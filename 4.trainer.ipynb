{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![4.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⌛For our training loop, you should:\n",
    "\n",
    "1. Create a save directory for your model if it doesn't exist already using `os.makedirs(save_path, exist_ok=True)`\n",
    "2. Move your model to your device (our Azure CPUs)\n",
    "3. Create the training loop\n",
    "    - `images` and `captions` as our input and output\n",
    "    - Zero graidents\n",
    "    - Forward pass\n",
    "    - Reshape our outputs (because our outputs are formatted like [seq_len, batch_size, vocab_size] when we want them to be [seq_len * batch_size, vocab_size])\n",
    "    - Calculate loss\n",
    "    - Backward pass\n",
    "    - Prevent exploding gradients with gradient clipping\n",
    "    - Update weights (`optimizer.step()`) and progress (`num_batch`, `epoch_loss`)\n",
    "    - Save checkpoints for each epoch and use the epoch with the smallest loss\n",
    "4. Per epoch, calculate average loss\n",
    "5. Using `plt`, plot the loss curve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm \n",
    "from torchvision import transforms\n",
    "from model import CNN_to_LSTM\n",
    "from preprocess import get_loader\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def train_loop(model, dataloader, loss_fn, optimizer, epochs, device, vocab_size, save_path=\"checkpoints\"):\n",
    "    \"\"\"\n",
    "    Simple training loop for CNN-LSTM multimodal model\n",
    "    \n",
    "    Args:\n",
    "        model: The CNN_to_LSTM model\n",
    "        dataloader: DataLoader containing images and captions\n",
    "        loss_fn: Loss function (CrossEntropyLoss)\n",
    "        optimizer: Optimizer (Adam)\n",
    "        epochs: Number of training epochs\n",
    "        device: Device to train on (cuda or cpu)\n",
    "        vocab_size: Size of vocabulary\n",
    "        save_path: Path to save model checkpoints\n",
    "    \"\"\"\n",
    "    # Create save directory if it doesn't exist\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    \n",
    "    # Track losses for plotting\n",
    "    losses = []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for images, captions in tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            images = images.to(device)\n",
    "            captions = captions.to(device)            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            # We need to provide both images and captions for teacher forcing\n",
    "            outputs = model(images, captions[:-1])  # We don't need the EOS token for input\n",
    "            \n",
    "            # Calculate loss\n",
    "            # The first sequence element in outputs corresponds to encoder features from the image\n",
    "            # So we need to skip it and only use predictions that correspond to actual words\n",
    "            \n",
    "            # Skip the first prediction (from encoder features) and use the rest for loss\n",
    "            # outputs shape: [seq_len, batch_size, vocab_size]\n",
    "            outputs = outputs[1:, :, :]  # Skip the first sequence element (from image features)\n",
    "            \n",
    "            # Reshape outputs to [seq_length * batch_size, vocab_size]\n",
    "            outputs = outputs.reshape(-1, vocab_size)\n",
    "            \n",
    "            # Get corresponding target captions starting from second token\n",
    "            # This aligns with outputs starting from second prediction\n",
    "            targets = captions[1:].reshape(-1)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping to prevent exploding gradients\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            num_batches += 1\n",
    "        \n",
    "        avg_epoch_loss = epoch_loss / num_batches\n",
    "        losses.append(avg_epoch_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_epoch_loss:.4f}\")\n",
    "        \n",
    "        # Save checkpoint after each epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_epoch_loss,\n",
    "        }, f\"{save_path}/model_epoch_{epoch+1}.pt\")\n",
    "        \n",
    "        # Save best model separately\n",
    "        if avg_epoch_loss < best_loss:\n",
    "            best_loss = avg_epoch_loss\n",
    "            torch.save(model, f\"{save_path}/best_model.pt\")\n",
    "            print(f\"New best model saved with loss: {best_loss:.4f}\")\n",
    "    \n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, epochs+1), losses, marker='o')\n",
    "    plt.title('Training Loss vs. Epochs')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.savefig(f\"{save_path}/training_loss.png\")\n",
    "    \n",
    "    # Save final model\n",
    "    torch.save(model, f\"{save_path}/final_model.pt\")\n",
    "    \n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to combine all the steps:\n",
    "  - 🟦Importing\n",
    "  - 🟧Data Preprocessing\n",
    "  - 🟩Multimodal AI Building\n",
    "  - 🟥Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Vocabulary size: 5240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50:   0%|          | 0/1265 [00:00<?, ?it/s]/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "Epoch 1/50:  16%|█▌        | 204/1265 [04:58<25:50,  1.46s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.003\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Train model\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining completed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal model saved to checkpoints/final_model.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 47\u001b[0m, in \u001b[0;36mtrain_loop\u001b[0;34m(model, dataloader, loss_fn, optimizer, epochs, device, vocab_size, save_path)\u001b[0m\n\u001b[1;32m     43\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# We need to provide both images and captions for teacher forcing\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# We don't need the EOS token for input\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# The first sequence element in outputs corresponds to encoder features from the image\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# So we need to skip it and only use predictions that correspond to actual words\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Skip the first prediction (from encoder features) and use the rest for loss\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;66;03m# outputs shape: [seq_len, batch_size, vocab_size]\u001b[39;00m\n\u001b[1;32m     55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m:, :, :]  \u001b[38;5;66;03m# Skip the first sequence element (from image features)\u001b[39;00m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/multimodal-ai-student/model.py:85\u001b[0m, in \u001b[0;36mCNN_to_LSTM.forward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, captions):\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m    Forward pass for training with teacher forcing\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     encoder_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m     outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(encoder_features, captions)\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/multimodal-ai-student/model.py:29\u001b[0m, in \u001b[0;36mCNN_Encoder.forward\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images):\n\u001b[0;32m---> 29\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresnet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;66;03m# Flatten the feature map/image\u001b[39;00m\n\u001b[1;32m     31\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflatten(features)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torchvision/models/resnet.py:152\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(out)\n\u001b[1;32m    151\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(out)\n\u001b[0;32m--> 152\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv3(out)\n\u001b[1;32m    155\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/modules/activation.py:133\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38/lib/python3.10/site-packages/torch/nn/functional.py:1702\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[1;32m   1701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m-> 1702\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1703\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1704\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define image preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Get data loader and dataset\n",
    "train_loader, dataset = get_loader(\n",
    "    root_dir=\"data/images/\", \n",
    "    captions_file=\"data/text.csv\", \n",
    "    transform=transform,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Print vocab size\n",
    "vocab_size = len(dataset.vocab)\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Initialize model\n",
    "model = CNN_to_LSTM(\n",
    "    embed_size=256,\n",
    "    hidden_size=512,\n",
    "    num_layers=2,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "# Define loss function and optimizer\n",
    "# Ignore padding index (0) in loss calculation\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=dataset.vocab.word_to_index[\"<PAD>\"])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "# Train model\n",
    "losses = train_loop(\n",
    "    model=model,\n",
    "    dataloader=train_loader,\n",
    "    loss_fn=loss_fn,\n",
    "    optimizer=optimizer,\n",
    "    epochs=50,\n",
    "    device=device,\n",
    "    vocab_size=vocab_size\n",
    ")\n",
    "\n",
    "print(\"Training completed!\")\n",
    "print(f\"Final model saved to checkpoints/final_model.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
